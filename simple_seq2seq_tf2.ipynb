{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_seq2seq_tf2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minus--/tensorflow-tutorials/blob/master/simple_seq2seq_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GecNn3N_yRXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sequence To Sequence Model in Tensorflow 2.0\n",
        "### French to English translation"
      ]
    },
    {
      "metadata": {
        "id": "_Xxa7dSIyhBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "5DQ2qNIBWOye",
        "colab_type": "code",
        "outputId": "25eead88-5136-40eb-de03-c10cc81c6d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python2.7/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.5)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y7E3LHdnynm3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Download and Processing"
      ]
    },
    {
      "metadata": {
        "id": "yD8mv1c4WT0_",
        "colab_type": "code",
        "outputId": "299b6393-b220-44ea-d6df-8507e067f537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the file\n",
        "!([ -f fra-eng.zip ] && rm *.zip && rm *.txt  || echo \"No existing file found\") && wget http://www.manythings.org/anki/fra-eng.zip && unzip fra-eng.zip\n",
        "\n",
        "path_to_zip = os.path.dirname(os.path.realpath('fra-eng.zip'))+\"/fra-eng.zip\"\n",
        "path_to_file = os.path.dirname(os.path.realpath('fra.txt'))+\"/fra.txt\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-11 08:04:17--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3423204 (3.3M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "\rfra-eng.zip           0%[                    ]       0  --.-KB/s               \rfra-eng.zip          39%[======>             ]   1.28M  6.39MB/s               \rfra-eng.zip         100%[===================>]   3.26M  13.0MB/s    in 0.3s    \n",
            "\n",
            "2019-04-11 08:04:18 (13.0 MB/s) - ‘fra-eng.zip’ saved [3423204/3423204]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ka3nBAeaWtWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZuM4nwRMvKe4",
        "colab_type": "code",
        "outputId": "265b6fae-9f66-42a9-ea5c-9fbbc0c7195f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "fr_sentence = u\"je peux vous empreinter un livre?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(fr_sentence).encode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "<start> je peux vous empreinter un livre ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFx2U_j6v5c0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, FRENCH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFT-QMgkw7Ew",
        "colab_type": "code",
        "outputId": "db4a63e3-de7b-438b-fb1b-aa93f11cee43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "en, fr = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(fr[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort . however , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>\n",
            "<start> il est peut etre impossible d obtenir un corpus completement denue de fautes , etant donnee la nature de ce type d entreprise collaborative . cependant , si nous encourageons les membres a produire des phrases dans leurs propres langues plutot que d experimenter dans les langues qu ils apprennent , nous pourrions etre en mesure de reduire les erreurs . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "akawn1S2xCma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pBUEvbEGxPFK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  \n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  \n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  \n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRPslMqyxTsr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Oq8a0TsxW4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eG49aKY-xcXB",
        "colab_type": "code",
        "outputId": "5b6d2598-7a8c-4a5a-e17c-44180e200eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "iMlGp3GjxfH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6fNN1wnxtg9",
        "colab_type": "code",
        "outputId": "1149cfd1-85e3-446a-e028-b39c8873af4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "22 ----> tom\n",
            "50 ----> aime\n",
            "17 ----> le\n",
            "4016 ----> fromage\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "16 ----> tom\n",
            "173 ----> likes\n",
            "2754 ----> cheese\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWs4MEwDxxer",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u5D5_lsJyybD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder Definition"
      ]
    },
    {
      "metadata": {
        "id": "Aa_hxDzFzhoq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    \n",
        "    self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state_h,state_c = self.lstm(x, initial_state=hidden)        \n",
        "    return output, state_h, state_c \n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)),tf.zeros((self.batch_sz, self.enc_units))]\n",
        "    #return self.lstm.zero_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCW8Chau58PH",
        "colab_type": "code",
        "outputId": "293457f8-a876-42ca-e960-56660c0b2d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 17]), TensorShape([64, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "7VBBG0HF53CY",
        "colab_type": "code",
        "outputId": "f6c8d15a-26df-48cb-f858-d27bac82f04e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden_h, sample_hidden_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden_h.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden_c.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0411 08:04:48.385406 140457399551872 recurrent.py:2914] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fbe4c044350>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 17, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v8nR-8Pfy2wc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder Definition"
      ]
    },
    {
      "metadata": {
        "id": "ENg4QxyV6F7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(self.dec_units, \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True, \n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):     \n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, h_state, c_state = self.lstm(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, h_state , c_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j20e9dg8-BjZ",
        "colab_type": "code",
        "outputId": "93d6cc19-2e98-43fc-fb88-dd7c4b65d2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)), \n",
        "                                      [sample_hidden_h, sample_hidden_c], sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0411 08:19:24.231101 140457399551872 recurrent.py:2914] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fbe4bb1bad0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4442)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M3_HSwIzy6Z1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Function"
      ]
    },
    {
      "metadata": {
        "id": "Y-Wi8mV0-Rud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSZw06MA_G73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RMA8T5-2y-XI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "WDES7fKAHafV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, teacher_forcing_ratio = 0.5):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)       \n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden_h, dec_hidden_c = decoder(dec_input, dec_hidden, enc_output)\n",
        "      dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      predict_ids = tf.expand_dims(tf.argmax(predictions, axis=1), 1)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1) if teacher_force else predict_ids\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  \n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPf8Hb3lzCxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the Training"
      ]
    },
    {
      "metadata": {
        "id": "x3qkesf7hr8j",
        "colab_type": "code",
        "outputId": "4e0c3968-881a-4bf6-ab15-2215395309c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3545
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.8929\n",
            "Epoch 1 Batch 100 Loss 2.1008\n",
            "Epoch 1 Batch 200 Loss 1.8143\n",
            "Epoch 1 Batch 300 Loss 1.8731\n",
            "Epoch 1 Loss 1.8189\n",
            "Time taken for 1 epoch 46.9882180691 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.7891\n",
            "Epoch 2 Batch 100 Loss 1.5851\n",
            "Epoch 2 Batch 200 Loss 1.5215\n",
            "Epoch 2 Batch 300 Loss 1.5720\n",
            "Epoch 2 Loss 1.5637\n",
            "Time taken for 1 epoch 47.5436458588 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.4696\n",
            "Epoch 3 Batch 100 Loss 1.3576\n",
            "Epoch 3 Batch 200 Loss 1.3232\n",
            "Epoch 3 Batch 300 Loss 1.3813\n",
            "Epoch 3 Loss 1.3596\n",
            "Time taken for 1 epoch 46.9673991203 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3267\n",
            "Epoch 4 Batch 100 Loss 1.1842\n",
            "Epoch 4 Batch 200 Loss 1.1846\n",
            "Epoch 4 Batch 300 Loss 1.2275\n",
            "Epoch 4 Loss 1.2164\n",
            "Time taken for 1 epoch 47.746309042 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.2127\n",
            "Epoch 5 Batch 100 Loss 1.0578\n",
            "Epoch 5 Batch 200 Loss 1.0743\n",
            "Epoch 5 Batch 300 Loss 1.1228\n",
            "Epoch 5 Loss 1.0968\n",
            "Time taken for 1 epoch 47.0672519207 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.1038\n",
            "Epoch 6 Batch 100 Loss 0.9467\n",
            "Epoch 6 Batch 200 Loss 0.9559\n",
            "Epoch 6 Batch 300 Loss 1.0094\n",
            "Epoch 6 Loss 0.9921\n",
            "Time taken for 1 epoch 47.4905278683 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0007\n",
            "Epoch 7 Batch 100 Loss 0.8602\n",
            "Epoch 7 Batch 200 Loss 0.8617\n",
            "Epoch 7 Batch 300 Loss 0.9099\n",
            "Epoch 7 Loss 0.9018\n",
            "Time taken for 1 epoch 47.1680819988 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8971\n",
            "Epoch 8 Batch 100 Loss 0.7866\n",
            "Epoch 8 Batch 200 Loss 0.7707\n",
            "Epoch 8 Batch 300 Loss 0.8390\n",
            "Epoch 8 Loss 0.8175\n",
            "Time taken for 1 epoch 47.6046471596 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8094\n",
            "Epoch 9 Batch 100 Loss 0.7013\n",
            "Epoch 9 Batch 200 Loss 0.6938\n",
            "Epoch 9 Batch 300 Loss 0.7539\n",
            "Epoch 9 Loss 0.7400\n",
            "Time taken for 1 epoch 47.1523909569 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.7307\n",
            "Epoch 10 Batch 100 Loss 0.6343\n",
            "Epoch 10 Batch 200 Loss 0.5734\n",
            "Epoch 10 Batch 300 Loss 0.6917\n",
            "Epoch 10 Loss 0.6756\n",
            "Time taken for 1 epoch 47.3510482311 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6663\n",
            "Epoch 11 Batch 100 Loss 0.6010\n",
            "Epoch 11 Batch 200 Loss 0.5335\n",
            "Epoch 11 Batch 300 Loss 0.6172\n",
            "Epoch 11 Loss 0.6174\n",
            "Time taken for 1 epoch 47.085119009 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.6064\n",
            "Epoch 12 Batch 100 Loss 0.5799\n",
            "Epoch 12 Batch 200 Loss 0.4681\n",
            "Epoch 12 Batch 300 Loss 0.6003\n",
            "Epoch 12 Loss 0.5621\n",
            "Time taken for 1 epoch 47.5385141373 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.5752\n",
            "Epoch 13 Batch 100 Loss 0.5025\n",
            "Epoch 13 Batch 200 Loss 0.4335\n",
            "Epoch 13 Batch 300 Loss 0.5048\n",
            "Epoch 13 Loss 0.5120\n",
            "Time taken for 1 epoch 47.1341109276 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.5085\n",
            "Epoch 14 Batch 100 Loss 0.4481\n",
            "Epoch 14 Batch 200 Loss 0.3800\n",
            "Epoch 14 Batch 300 Loss 0.5283\n",
            "Epoch 14 Loss 0.4663\n",
            "Time taken for 1 epoch 47.4214069843 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4756\n",
            "Epoch 15 Batch 100 Loss 0.4071\n",
            "Epoch 15 Batch 200 Loss 0.3439\n",
            "Epoch 15 Batch 300 Loss 0.4128\n",
            "Epoch 15 Loss 0.4241\n",
            "Time taken for 1 epoch 47.2349860668 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3977\n",
            "Epoch 16 Batch 100 Loss 0.3916\n",
            "Epoch 16 Batch 200 Loss 0.3344\n",
            "Epoch 16 Batch 300 Loss 0.3827\n",
            "Epoch 16 Loss 0.3870\n",
            "Time taken for 1 epoch 47.8133220673 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.3665\n",
            "Epoch 17 Batch 100 Loss 0.3675\n",
            "Epoch 17 Batch 200 Loss 0.2890\n",
            "Epoch 17 Batch 300 Loss 0.3652\n",
            "Epoch 17 Loss 0.3522\n",
            "Time taken for 1 epoch 47.1585030556 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.3291\n",
            "Epoch 18 Batch 100 Loss 0.3543\n",
            "Epoch 18 Batch 200 Loss 0.2396\n",
            "Epoch 18 Batch 300 Loss 0.3089\n",
            "Epoch 18 Loss 0.3186\n",
            "Time taken for 1 epoch 48.575646162 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2862\n",
            "Epoch 19 Batch 100 Loss 0.3015\n",
            "Epoch 19 Batch 200 Loss 0.2320\n",
            "Epoch 19 Batch 300 Loss 0.2979\n",
            "Epoch 19 Loss 0.2863\n",
            "Time taken for 1 epoch 47.113683939 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.2853\n",
            "Epoch 20 Batch 100 Loss 0.2670\n",
            "Epoch 20 Batch 200 Loss 0.2026\n",
            "Epoch 20 Batch 300 Loss 0.2678\n",
            "Epoch 20 Loss 0.2619\n",
            "Time taken for 1 epoch 48.0769081116 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.2746\n",
            "Epoch 21 Batch 100 Loss 0.2592\n",
            "Epoch 21 Batch 200 Loss 0.1934\n",
            "Epoch 21 Batch 300 Loss 0.2450\n",
            "Epoch 21 Loss 0.2365\n",
            "Time taken for 1 epoch 47.1151659489 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2366\n",
            "Epoch 22 Batch 100 Loss 0.2315\n",
            "Epoch 22 Batch 200 Loss 0.1801\n",
            "Epoch 22 Batch 300 Loss 0.2137\n",
            "Epoch 22 Loss 0.2134\n",
            "Time taken for 1 epoch 47.5630440712 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2123\n",
            "Epoch 23 Batch 100 Loss 0.2290\n",
            "Epoch 23 Batch 200 Loss 0.1526\n",
            "Epoch 23 Batch 300 Loss 0.2125\n",
            "Epoch 23 Loss 0.1914\n",
            "Time taken for 1 epoch 46.9435210228 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1996\n",
            "Epoch 24 Batch 100 Loss 0.1918\n",
            "Epoch 24 Batch 200 Loss 0.1406\n",
            "Epoch 24 Batch 300 Loss 0.1666\n",
            "Epoch 24 Loss 0.1727\n",
            "Time taken for 1 epoch 47.5706400871 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1945\n",
            "Epoch 25 Batch 100 Loss 0.1892\n",
            "Epoch 25 Batch 200 Loss 0.1385\n",
            "Epoch 25 Batch 300 Loss 0.2150\n",
            "Epoch 25 Loss 0.1590\n",
            "Time taken for 1 epoch 47.149520874 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.1486\n",
            "Epoch 26 Batch 100 Loss 0.1570\n",
            "Epoch 26 Batch 200 Loss 0.1170\n",
            "Epoch 26 Batch 300 Loss 0.1639\n",
            "Epoch 26 Loss 0.1431\n",
            "Time taken for 1 epoch 47.6534790993 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1389\n",
            "Epoch 27 Batch 100 Loss 0.1437\n",
            "Epoch 27 Batch 200 Loss 0.0971\n",
            "Epoch 27 Batch 300 Loss 0.1372\n",
            "Epoch 27 Loss 0.1318\n",
            "Time taken for 1 epoch 47.1376850605 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.1324\n",
            "Epoch 28 Batch 100 Loss 0.1619\n",
            "Epoch 28 Batch 200 Loss 0.1255\n",
            "Epoch 28 Batch 300 Loss 0.1265\n",
            "Epoch 28 Loss 0.1180\n",
            "Time taken for 1 epoch 47.6142511368 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.1240\n",
            "Epoch 29 Batch 100 Loss 0.1358\n",
            "Epoch 29 Batch 200 Loss 0.0697\n",
            "Epoch 29 Batch 300 Loss 0.1296\n",
            "Epoch 29 Loss 0.1041\n",
            "Time taken for 1 epoch 46.9961519241 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.1242\n",
            "Epoch 30 Batch 100 Loss 0.1365\n",
            "Epoch 30 Batch 200 Loss 0.0916\n",
            "Epoch 30 Batch 300 Loss 0.1507\n",
            "Epoch 30 Loss 0.0986\n",
            "Time taken for 1 epoch 48.190101862 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NwZund7wzH1F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Results"
      ]
    },
    {
      "metadata": {
        "id": "yobqGXS-hxf1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], \n",
        "                                                           maxlen=max_length_inp, \n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden_h, enc_hidden_c  = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    \n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden_h, dec_hidden_c = decoder(dec_input, \n",
        "                                                             dec_hidden, \n",
        "                                                             enc_out)\n",
        "        dec_hidden = [dec_hidden_h, dec_hidden_c]\n",
        "        \n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G_d6bYyBnhHC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "        \n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XLAgIVYZnra2",
        "colab_type": "code",
        "outputId": "cb6d720f-acf5-4d11-f54a-861efa52b543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbe4a0b0b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "rdQHno1BnuMa",
        "colab_type": "code",
        "outputId": "6a918e55-95c5-4885-e5a6-58b10a723b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "translate(u\"c'est ma vie\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> c est ma vie <end>\n",
            "Predicted translation: that s my sister . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "979ejdx9n3Sm",
        "colab_type": "code",
        "outputId": "1ffaf304-bd55-4ab9-8621-66e7f03b317b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "translate(u\"je suis a la maison\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> je suis a la maison <end>\n",
            "Predicted translation: i m at home . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HCQS-QiCuU0U",
        "colab_type": "code",
        "outputId": "d41e100c-d0cb-42d7-dcc6-fc6d856b7a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "translate(u\"je suis dans les environs\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> je suis dans les environs <end>\n",
            "Predicted translation: i m on strike . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-3CCHsscueqi",
        "colab_type": "code",
        "outputId": "a65edd6a-0dbb-4dab-e066-fa6cae2cce99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "translate(u\"tu es encore a la maison?\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> tu es encore a la maison ? <end>\n",
            "Predicted translation: whose son is tom ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ZBz2kWgukSu",
        "colab_type": "code",
        "outputId": "605d3f91-5ca8-4ba7-adc0-00f35d144559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "translate(u\"il fait tres froid\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> il fait tres froid <end>\n",
            "Predicted translation: it s very cold . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vs5uoUuqusju",
        "colab_type": "code",
        "outputId": "1e135586-cb39-47e0-c843-9755864aafad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "translate(\"je peux vous acheter un livre ?\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> je peux vous acheter un livre ? <end>\n",
            "Predicted translation: can i help you you ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YEsLH53T8szF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}